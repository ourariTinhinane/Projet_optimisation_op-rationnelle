{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be348692-61a1-4a4d-9d6e-cd59c3f45f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RÉSUMÉ ===\n",
      "{'sheet_names': ['reseau_en_arbre'], 'loaded_sheet': 'reseau_en_arbre', 'row_count': 6107, 'columns': ['id_batiment', 'nb_maisons', 'infra_id', 'infra_type', 'longueur'], 'inferred_columns': {'source': None, 'target': None, 'cost_or_length': 'longueur', 'status': None}}\n",
      "\n",
      "=== APERÇU (30 premières lignes) ===\n",
      "   id_batiment  nb_maisons infra_id     infra_type   longueur\n",
      "0      E000001           4  P007111  infra_intacte  12.314461\n",
      "1      E000001           4  P007983  infra_intacte  40.320929\n",
      "2      E000001           4  P000308  infra_intacte  39.140799\n",
      "3      E000001           4  P007819  infra_intacte  17.390464\n",
      "4      E000002           1  P007111  infra_intacte  12.314461\n",
      "5      E000002           1  P007983  infra_intacte  40.320929\n",
      "6      E000002           1  P000308  infra_intacte  39.140799\n",
      "7      E000002           1  P007240  infra_intacte  10.914626\n",
      "8      E000003           1  P007111  infra_intacte  12.314461\n",
      "9      E000003           1  P007111  infra_intacte  12.314461\n",
      "10     E000003           1  P007113    a_remplacer  12.193509\n",
      "11     E000003           1  P007984    a_remplacer  30.057598\n",
      "12     E000003           1  P007823    a_remplacer  12.290283\n",
      "13     E000004           1  P007111  infra_intacte  12.314461\n",
      "14     E000004           1  P007983  infra_intacte  40.320929\n",
      "15     E000004           1  P000308  infra_intacte  39.140799\n",
      "16     E000004           1  P000378  infra_intacte  27.481806\n",
      "17     E000004           1  P007393  infra_intacte  10.635525\n",
      "18     E000005           1  P007111  infra_intacte  12.314461\n",
      "19     E000005           1  P007983  infra_intacte  40.320929\n",
      "20     E000005           1  P000308  infra_intacte  39.140799\n",
      "21     E000005           1  P000378  infra_intacte  27.481806\n",
      "22     E000005           1  P007249  infra_intacte  14.643782\n",
      "23     E000006           1  P007111  infra_intacte  12.314461\n",
      "24     E000006           1  P007983  infra_intacte  40.320929\n",
      "25     E000006           1  P000308  infra_intacte  39.140799\n",
      "26     E000006           1  P000308  infra_intacte  39.140799\n",
      "27     E000006           1  P007173  infra_intacte   8.702344\n",
      "28     E000007           1  P007111  infra_intacte  12.314461\n",
      "29     E000007           1  P007111  infra_intacte  12.314461\n",
      "\n",
      "CSV nettoyé sauvegardé → C:\\Users\\MicroMedia\\Desktop\\projet_r&d\\reseau_en_arbre_clean.csv\n",
      "Colonnes source/target non trouvées (normal si le fichier ne les contient pas).\n"
     ]
    }
   ],
   "source": [
    "# === Étape 1 — Exploration & nettoyage du fichier reseau_en_arbre.xlsx ===\n",
    "# - Liste les onglets\n",
    "# - Charge l’onglet (par défaut le 1er)\n",
    "# - Normalise les noms de colonnes\n",
    "# - Infère (si possible) les colonnes source/target/cost/status\n",
    "# - Affiche un aperçu\n",
    "# - Sauvegarde un CSV nettoyé\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Chemin du fichier (adapte si besoin)\n",
    "XLSX_PATH = Path(\"reseau_en_arbre.xlsx\")  # ex: Path(\"/mnt/data/reseau_en_arbre.xlsx\")\n",
    "\n",
    "assert XLSX_PATH.exists(), f\"Fichier introuvable: {XLSX_PATH.resolve()}\"\n",
    "\n",
    "# 1) Lire les onglets et choisir celui à charger\n",
    "xls = pd.ExcelFile(XLSX_PATH)\n",
    "sheet_names = xls.sheet_names\n",
    "SHEET_TO_READ = sheet_names[0]  # change si besoin: SHEET_TO_READ = \"reseau_en_arbre\"\n",
    "\n",
    "# 2) Charger l’onglet\n",
    "df_raw = pd.read_excel(XLSX_PATH, sheet_name=SHEET_TO_READ)\n",
    "\n",
    "# 3) Normaliser les noms de colonnes\n",
    "def normalize(col: str) -> str:\n",
    "    return (\n",
    "        str(col)\n",
    "        .strip()\n",
    "        .lower()\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\".\", \"_\")\n",
    "        .replace(\"/\", \"_\")\n",
    "    )\n",
    "\n",
    "df = df_raw.copy()\n",
    "df.columns = [normalize(c) for c in df.columns]\n",
    "\n",
    "# 4) Inférence (facultative) de colonnes de graphe\n",
    "def pick(cols, options):\n",
    "    for o in options:\n",
    "        if o in cols:\n",
    "            return o\n",
    "    return None\n",
    "\n",
    "source_col = pick(df.columns, [\"source\",\"from\",\"src\",\"noeud_source\",\"node_source\",\"origin\",\"arc_debut\",\"noeud_debut\",\"sommet_debut\",\"u\",\"depart\"])\n",
    "target_col = pick(df.columns, [\"target\",\"to\",\"dst\",\"noeud_cible\",\"node_target\",\"destination\",\"arc_fin\",\"noeud_fin\",\"sommet_fin\",\"v\",\"arrivee\",\"arrivée\"])\n",
    "cost_col   = pick(df.columns, [\"cost\",\"cout\",\"coût\",\"weight\",\"poids\",\"length\",\"longueur\",\"distance\",\"prix\"])\n",
    "status_col = pick(df.columns, [\"status\",\"statut\",\"etat\",\"état\",\"state\",\"condition\",\"type_etat\",\"etat_arc\"])\n",
    "\n",
    "summary = {\n",
    "    \"sheet_names\": sheet_names,\n",
    "    \"loaded_sheet\": SHEET_TO_READ,\n",
    "    \"row_count\": len(df),\n",
    "    \"columns\": list(df.columns),\n",
    "    \"inferred_columns\": {\n",
    "        \"source\": source_col,\n",
    "        \"target\": target_col,\n",
    "        \"cost_or_length\": cost_col,\n",
    "        \"status\": status_col,\n",
    "    },\n",
    "}\n",
    "\n",
    "# 5) Afficher un aperçu\n",
    "print(\"=== RÉSUMÉ ===\")\n",
    "print(summary)\n",
    "print(\"\\n=== APERÇU (30 premières lignes) ===\")\n",
    "display(df.head(30)) if \"display\" in globals() else print(df.head(30))\n",
    "\n",
    "# 6) Sauvegarder un CSV nettoyé pour les prochaines étapes\n",
    "CLEAN_CSV_PATH = XLSX_PATH.with_suffix(\"\").with_name(\"reseau_en_arbre_clean.csv\")\n",
    "df.to_csv(CLEAN_CSV_PATH, index=False)\n",
    "print(f\"\\nCSV nettoyé sauvegardé → {CLEAN_CSV_PATH.resolve()}\")\n",
    "\n",
    "# 7) Diagnostics simples (si source/target existent)\n",
    "if source_col and target_col:\n",
    "    unique_nodes = pd.unique(pd.concat([df[source_col], df[target_col]], ignore_index=True)).size\n",
    "    print({\"unique_nodes_estimate\": unique_nodes, \"edge_count\": len(df)})\n",
    "else:\n",
    "    print(\"Colonnes source/target non trouvées (normal si le fichier ne les contient pas).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e971d5-2035-4630-8113-685366b2ee24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Graphe biparti ===\n",
      "- Nœuds: 1025 | Arêtes: 5586\n",
      "\n",
      "=== Graphe bâtiments-uniquement ===\n",
      "- Nœuds: 381 | Arêtes: 533\n",
      "- Composantes connexes: 1\n",
      "- Plus grande composante: 381 nœuds\n",
      "Export GraphML (optionnel) ignoré: GraphML does not support type <class 'set'> as data values.\n",
      "\n",
      "Fichiers générés:\n",
      "- edges_building_graph.csv\n",
      "- edges_bipartite_graph.csv\n"
     ]
    }
   ],
   "source": [
    "# === Étape 2 — Construire les graphes NetworkX ===\n",
    "# - G_bip (biparti): noeuds={batiments, infras}, arêtes=(batiment <-> infra)\n",
    "# - G_building (projection): noeuds=bâtiments, arêtes=lignes électriques (mutualisées)\n",
    "#   Poids = coût du raccordement (par défaut: longueur ; option “per_house” possible)\n",
    "#\n",
    "# Entrée : reseau_en_arbre_clean.csv avec colonnes attendues :\n",
    "#   id_batiment, nb_maisons, infra_id, infra_type, longueur\n",
    "#\n",
    "# Sorties :\n",
    "#   - Diagnostics imprimés (taille, composantes)\n",
    "#   - Deux fichiers CSV d’arêtes pour visualiser ou recharger plus tard:\n",
    "#       edges_building_graph.csv  (bâtiments seulement)\n",
    "#       edges_bipartite_graph.csv (biparti bâtiment<->infra)\n",
    "#   - (optionnel) graphes GraphML :\n",
    "#       G_building.graphml, G_bip.graphml\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# 0) Paramètres\n",
    "CLEAN_CSV_PATH = Path(\"reseau_en_arbre_clean.csv\")   # adapte si besoin (ex: /mnt/data/...)\n",
    "assert CLEAN_CSV_PATH.exists(), f\"Fichier introuvable: {CLEAN_CSV_PATH.resolve()}\"\n",
    "\n",
    "# Mode de coût (poids de l’arête dans le graphe Bâtiments)\n",
    "#   \"length\"   -> coût = longueur totale, répartie équitablement sur (k-1) arêtes du groupe\n",
    "#   \"per_house\"-> coût = (longueur / nb_maisons), répartie sur (k-1) arêtes\n",
    "COST_MODE = \"length\"   # \"length\" ou \"per_house\"\n",
    "\n",
    "df = pd.read_csv(CLEAN_CSV_PATH)\n",
    "\n",
    "required_cols = {\"id_batiment\", \"nb_maisons\", \"infra_id\", \"infra_type\", \"longueur\"}\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes: {missing}\")\n",
    "\n",
    "# ——————————————————————————————————————————————\n",
    "# 1) Graphe biparti Bâtiment <-> Infrastructure\n",
    "# ——————————————————————————————————————————————\n",
    "G_bip = nx.Graph()\n",
    "\n",
    "# Ajout des nœuds bâtiments + infras\n",
    "# On met un attribut \"bipartite\" pour distinguer les deux ensembles\n",
    "for bid in df[\"id_batiment\"].unique():\n",
    "    G_bip.add_node((\"B\", bid), kind=\"building\", bipartite=\"building\")\n",
    "\n",
    "for iid, row in df.drop_duplicates(\"infra_id\")[[\"infra_id\",\"infra_type\"]].itertuples(index=False):\n",
    "    G_bip.add_node((\"I\", iid), kind=\"infra\", infra_type=row, bipartite=\"infra\")\n",
    "\n",
    "# Ajout des arêtes bâtiment <-> infra\n",
    "# Attributs utiles: longueur, nb_maisons, infra_type\n",
    "for _, r in df.iterrows():\n",
    "    b_node = (\"B\", r[\"id_batiment\"])\n",
    "    i_node = (\"I\", r[\"infra_id\"])\n",
    "    G_bip.add_edge(\n",
    "        b_node, i_node,\n",
    "        longueur=float(r[\"longueur\"]),\n",
    "        nb_maisons=int(r[\"nb_maisons\"]) if pd.notna(r[\"nb_maisons\"]) else None,\n",
    "        infra_type=r[\"infra_type\"]\n",
    "    )\n",
    "\n",
    "# Exporte l’edge list bipartite pour inspection\n",
    "edges_bip = []\n",
    "for u, v, d in G_bip.edges(data=True):\n",
    "    edges_bip.append({\n",
    "        \"u\": u[1], \"u_kind\": u[0],\n",
    "        \"v\": v[1], \"v_kind\": v[0],\n",
    "        \"longueur\": d.get(\"longueur\"),\n",
    "        \"nb_maisons\": d.get(\"nb_maisons\"),\n",
    "        \"infra_type\": d.get(\"infra_type\")\n",
    "    })\n",
    "edges_bip_df = pd.DataFrame(edges_bip)\n",
    "edges_bip_df.to_csv(\"edges_bipartite_graph.csv\", index=False)\n",
    "\n",
    "# ——————————————————————————————————————————————\n",
    "# 2) Graphe Bâtiments-uniquement (projection)\n",
    "#    Principe : pour chaque infra_id, relier entre eux les bâtiments qui la partagent.\n",
    "#    Pour éviter des cliques denses, on crée une simple chaîne (k-1 arêtes) en triant les id_batiment.\n",
    "#    Le coût de l’infra est réparti uniformément sur ces (k-1) arêtes.\n",
    "# ——————————————————————————————————————————————\n",
    "G_building = nx.Graph()\n",
    "\n",
    "# Ajoute tous les bâtiments comme nœuds (avec attributs éventuels)\n",
    "for bid in df[\"id_batiment\"].unique():\n",
    "    # On peut stocker le nb_maisons max observé pour ce bâtiment (si utile)\n",
    "    nb_maisons_b = df.loc[df[\"id_batiment\"] == bid, \"nb_maisons\"].max()\n",
    "    G_building.add_node(bid, nb_maisons=int(nb_maisons_b) if pd.notna(nb_maisons_b) else None)\n",
    "\n",
    "# Regroupe par infra\n",
    "for infra_id, g in df.groupby(\"infra_id\"):\n",
    "    batts = sorted(g[\"id_batiment\"].unique().tolist())\n",
    "    k = len(batts)\n",
    "    if k < 2:\n",
    "        # Une infra liée à un seul bâtiment ne crée pas d’arête dans le graphe bâtiment-bâtiment\n",
    "        continue\n",
    "\n",
    "    # Coût (longueur) total de l’infra\n",
    "    longueur_total = float(g[\"longueur\"].sum())\n",
    "    # Nombre de maisons max sur cette infra (proxy de mutualisation)\n",
    "    nb_maisons_max = int(g[\"nb_maisons\"].max()) if pd.notna(g[\"nb_maisons\"].max()) else 1\n",
    "\n",
    "    # Répartition du coût sur (k-1) arêtes formant une chaîne\n",
    "    denom = max(1, k - 1)\n",
    "\n",
    "    if COST_MODE == \"length\":\n",
    "        cost_edge = longueur_total / denom\n",
    "    elif COST_MODE == \"per_house\":\n",
    "        # coût “par maison” réparti sur les arêtes\n",
    "        cost_edge = (longueur_total / max(1, nb_maisons_max)) / denom\n",
    "    else:\n",
    "        raise ValueError(\"COST_MODE invalide. Utilise 'length' ou 'per_house'.\")\n",
    "\n",
    "    # Construire la chaîne\n",
    "    for a, b in zip(batts[:-1], batts[1:]):\n",
    "        # Si une arête existe déjà (via une autre infra), on cumule les coûts\n",
    "        if G_building.has_edge(a, b):\n",
    "            G_building[a][b][\"cost\"] += cost_edge\n",
    "            G_building[a][b][\"infras\"].add(infra_id)\n",
    "            G_building[a][b][\"longueur_total\"] += longueur_total / denom\n",
    "        else:\n",
    "            G_building.add_edge(\n",
    "                a, b,\n",
    "                cost=cost_edge,\n",
    "                infras={infra_id},\n",
    "                longueur_total=longueur_total / denom,\n",
    "                k_group=k,\n",
    "                nb_maisons_max=nb_maisons_max\n",
    "            )\n",
    "\n",
    "# ——————————————————————————————————————————————\n",
    "# 3) Diagnostics & export\n",
    "# ——————————————————————————————————————————————\n",
    "print(\"=== Graphe biparti ===\")\n",
    "print(f\"- Nœuds: {G_bip.number_of_nodes()} | Arêtes: {G_bip.number_of_edges()}\")\n",
    "\n",
    "print(\"\\n=== Graphe bâtiments-uniquement ===\")\n",
    "print(f\"- Nœuds: {G_building.number_of_nodes()} | Arêtes: {G_building.number_of_edges()}\")\n",
    "\n",
    "# Composantes connexes (bâtiments)\n",
    "components = list(nx.connected_components(G_building))\n",
    "print(f\"- Composantes connexes: {len(components)}\")\n",
    "if components:\n",
    "    largest = max(components, key=len)\n",
    "    print(f\"- Plus grande composante: {len(largest)} nœuds\")\n",
    "\n",
    "# Export edge list pour G_building\n",
    "edges_building = []\n",
    "for u, v, d in G_building.edges(data=True):\n",
    "    edges_building.append({\n",
    "        \"u\": u,\n",
    "        \"v\": v,\n",
    "        \"cost\": d.get(\"cost\"),\n",
    "        \"longueur_total_share\": d.get(\"longueur_total\"),\n",
    "        \"nb_infras_partagees\": len(d.get(\"infras\", [])),\n",
    "        \"k_group\": d.get(\"k_group\"),\n",
    "        \"nb_maisons_max\": d.get(\"nb_maisons_max\"),\n",
    "    })\n",
    "edges_building_df = pd.DataFrame(edges_building).sort_values(\"cost\", ascending=True)\n",
    "edges_building_df.to_csv(\"edges_building_graph.csv\", index=False)\n",
    "\n",
    "# (Optionnel) Sauvegarde en GraphML pour Gephi / yEd / etc.\n",
    "try:\n",
    "    nx.write_graphml(G_building, \"G_building.graphml\")\n",
    "    nx.write_graphml(G_bip, \"G_bip.graphml\")\n",
    "    print(\"\\nGraphML exportés: G_building.graphml, G_bip.graphml\")\n",
    "except Exception as e:\n",
    "    print(f\"Export GraphML (optionnel) ignoré: {e}\")\n",
    "\n",
    "print(\"\\nFichiers générés:\")\n",
    "print(\"- edges_building_graph.csv\")\n",
    "print(\"- edges_bipartite_graph.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f435c405-b140-4a32-bb1a-85397b3b5f28",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'infrastructures_mutualisees.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m infra = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minfrastructures_mutualisees.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m top_n = \u001b[32m20\u001b[39m  \u001b[38;5;66;03m# ajuste si besoin\u001b[39;00m\n\u001b[32m      7\u001b[39m top = infra.sort_values([\u001b[33m\"\u001b[39m\u001b[33mnb_batiments\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdifficulty_infra\u001b[39m\u001b[33m\"\u001b[39m], ascending=[\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m]).head(top_n)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'infrastructures_mutualisees.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "infra = pd.read_csv(\"infrastructures_mutualisees.csv\")\n",
    "\n",
    "top_n = 20  # ajuste si besoin\n",
    "top = infra.sort_values([\"nb_batiments\", \"difficulty_infra\"], ascending=[False, True]).head(top_n)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(top[\"infra_id\"].astype(str), top[\"nb_batiments\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(f\"Top {top_n} infrastructures par nb de bâtiments desservis\")\n",
    "plt.xlabel(\"infra_id\")\n",
    "plt.ylabel(\"Nombre de bâtiments\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0951698-f796-4f1a-8e10-95a7e479d3ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'E000001'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 218\u001b[39m\n\u001b[32m    215\u001b[39m df = pd.read_csv(CSV)\n\u001b[32m    217\u001b[39m \u001b[38;5;66;03m# 2) Construire le réseau (tu peux ajuster α, β, γ)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m net = \u001b[43mNetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha_replace\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# pénalise plus les infras \"à remplacer\"\u001b[39;49;00m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pénalise un peu plus \"complexe/difficile\"\u001b[39;49;00m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma_simple\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# bonus (réduction) si \"simple\"\u001b[39;49;00m\n\u001b[32m    223\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28mprint\u001b[39m(net)  \u001b[38;5;66;03m# __repr__\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# 3) Générer le plan (POO)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 154\u001b[39m, in \u001b[36mNetwork.from_dataframe\u001b[39m\u001b[34m(cls, df, alpha_replace, beta_complex, gamma_simple)\u001b[39m\n\u001b[32m    152\u001b[39m b_to_i: Dict[\u001b[38;5;28mint\u001b[39m, Set[\u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mint\u001b[39m]] = {}\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m dff[[\u001b[33m\"\u001b[39m\u001b[33mid_batiment\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33minfra_id\u001b[39m\u001b[33m\"\u001b[39m]].drop_duplicates().itertuples(index=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     b_to_i.setdefault(\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid_batiment\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mset\u001b[39m()).add(r.infra_id)\n\u001b[32m    155\u001b[39m     infras[r.infra_id].batiments.add(\u001b[38;5;28mint\u001b[39m(r.id_batiment))\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# créer objets Building\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: 'E000001'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Plan de raccordement — Version Orientée Objet\n",
    "# ============================================\n",
    "# Hypothèses de colonnes (df): id_batiment, nb_maisons, infra_id, infra_type, longueur\n",
    "# Métriques (adaptables) :\n",
    "#   - difficulté(infra) = longueur * (1 + α*replace) * (1 + β*complex) * (1 - γ*simple)\n",
    "#   - difficulté(bâtiment) = somme des difficultés des infras non réparées qui le raccordent\n",
    "# Algorithme (itératif, commence par le plus facile) :\n",
    "#   - choisir le bâtiment avec difficulté min\n",
    "#   - \"réparer\" ses infras (elles sortent du calcul)\n",
    "#   - réévaluer difficultés des voisins\n",
    "#   - répéter\n",
    "#\n",
    "# Dunder methods:\n",
    "#   - __repr__/__str__ pour affichage\n",
    "#   - __lt__ pour trier (ordre par difficulté puis tie-break)\n",
    "#   - @classmethod from_dataframe(...) pour construire l'objet réseau\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Set, List, Tuple, Optional, Iterable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# Petites fonctions utilitaires\n",
    "# -------------------------\n",
    "def parse_flags(infra_type: str) -> Tuple[int,int,int]:\n",
    "    \"\"\"Détecte 'remplac', 'simple', 'complex/difficil' dans infra_type.\"\"\"\n",
    "    s = str(infra_type).lower() if pd.notna(infra_type) else \"\"\n",
    "    replace_flag = int(\"remplac\" in s)\n",
    "    simple_flag  = int(\"simple\" in s)\n",
    "    complex_flag = int((\"complex\" in s) or (\"difficil\" in s))\n",
    "    return replace_flag, simple_flag, complex_flag\n",
    "\n",
    "\n",
    "@dataclass(order=False)\n",
    "class Infrastructure:\n",
    "    id: int | str\n",
    "    longueur_total: float\n",
    "    replace_any: int = 0\n",
    "    simple_any: int = 0\n",
    "    complex_any: int = 0\n",
    "    batiments: Set[int] = field(default_factory=set)\n",
    "\n",
    "    # paramètres de difficulté (globaux mais passés ici pour flexibilité)\n",
    "    alpha_replace: float = 1.0   # pénalité si à remplacer\n",
    "    beta_complex: float  = 0.5   # pénalité si complexe/difficile\n",
    "    gamma_simple: float  = 0.3   # bonus si simple (réduit la difficulté)\n",
    "\n",
    "    def difficulty(self) -> float:\n",
    "        base = max(0.0, float(self.longueur_total))\n",
    "        mult = (1.0 + self.alpha_replace*self.replace_any) \\\n",
    "             * (1.0 + self.beta_complex*self.complex_any) \\\n",
    "             * (1.0 - self.gamma_simple*self.simple_any)\n",
    "        return max(0.0, base * mult)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"Infrastructure(id={self.id}, L={self.longueur_total:.2f}, \"\n",
    "                f\"rep={self.replace_any}, simp={self.simple_any}, cmp={self.complex_any}, \"\n",
    "                f\"bats={len(self.batiments)})\")\n",
    "\n",
    "\n",
    "@dataclass(order=False)\n",
    "class Building:\n",
    "    id: int\n",
    "    nb_maisons: Optional[int] = None\n",
    "    # références vers le réseau pour accéder aux infras non réparées\n",
    "    _network: \"Network\" = field(default=None, repr=False, compare=False)\n",
    "\n",
    "    # tie-break: favoriser zones avec bcp de remplacements / longueurs\n",
    "    tb_w_replace: float = 1.0\n",
    "    tb_w_length: float  = 0.2\n",
    "\n",
    "    def current_infras(self) -> Iterable[Infrastructure]:\n",
    "        for iid in self._network.b_to_i.get(self.id, set()):\n",
    "            if iid in self._network.unrepaired:\n",
    "                yield self._network.infras[iid]\n",
    "\n",
    "    def difficulty(self) -> float:\n",
    "        return float(sum(infra.difficulty() for infra in self.current_infras()))\n",
    "\n",
    "    def tie_break(self) -> float:\n",
    "        rep = 0.0\n",
    "        length = 0.0\n",
    "        for infra in self.current_infras():\n",
    "            rep    += float(infra.replace_any)\n",
    "            length += float(infra.longueur_total)\n",
    "        return self.tb_w_replace * rep + self.tb_w_length * length\n",
    "\n",
    "    # dunder pour tri : plus facile d'abord ; si égalité => tie-break décroissant\n",
    "    def __lt__(self, other: \"Building\") -> bool:\n",
    "        d1, d2 = self.difficulty(), other.difficulty()\n",
    "        if not np.isclose(d1, d2):\n",
    "            return d1 < d2\n",
    "        return self.tie_break() > other.tie_break()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Building(id={self.id}, diff={self.difficulty():.3f}, tb={self.tie_break():.3f})\"\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self,\n",
    "                 buildings: Dict[int, Building],\n",
    "                 infras: Dict[str | int, Infrastructure],\n",
    "                 b_to_i: Dict[int, Set[str | int]]):\n",
    "        self.buildings = buildings\n",
    "        self.infras = infras\n",
    "        self.b_to_i = b_to_i\n",
    "        self.unrepaired: Set[str | int] = set(infras.keys())\n",
    "        # back-reference\n",
    "        for b in self.buildings.values():\n",
    "            b._network = self\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls,\n",
    "                       df: pd.DataFrame,\n",
    "                       alpha_replace: float = 1.0,\n",
    "                       beta_complex: float  = 0.5,\n",
    "                       gamma_simple: float  = 0.3) -> \"Network\":\n",
    "        required = {\"id_batiment\",\"infra_id\",\"infra_type\",\"longueur\"}\n",
    "        missing = [c for c in required if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Colonnes manquantes: {missing}\")\n",
    "\n",
    "        # flags & agrégation des infras\n",
    "        flags = df[\"infra_type\"].apply(parse_flags).apply(pd.Series)\n",
    "        flags.columns = [\"replace_flag\",\"simple_flag\",\"complex_flag\"]\n",
    "        dff = pd.concat([df.copy(), flags], axis=1)\n",
    "\n",
    "        agg = (dff.groupby(\"infra_id\", as_index=False)\n",
    "                     .agg(longueur_total=(\"longueur\",\"sum\"),\n",
    "                          replace_any=(\"replace_flag\",\"max\"),\n",
    "                          simple_any=(\"simple_flag\",\"max\"),\n",
    "                          complex_any=(\"complex_flag\",\"max\")))\n",
    "        # créer objets Infrastructure\n",
    "        infras: Dict[str | int, Infrastructure] = {}\n",
    "        for row in agg.itertuples(index=False):\n",
    "            infras[row.infra_id] = Infrastructure(\n",
    "                id=row.infra_id,\n",
    "                longueur_total=float(row.longueur_total),\n",
    "                replace_any=int(row.replace_any),\n",
    "                simple_any=int(row.simple_any),\n",
    "                complex_any=int(row.complex_any),\n",
    "                batiments=set(),\n",
    "                alpha_replace=alpha_replace,\n",
    "                beta_complex=beta_complex,\n",
    "                gamma_simple=gamma_simple\n",
    "            )\n",
    "\n",
    "        # mapping bâtiment -> infras, et rattacher bâtiments aux infras\n",
    "        b_to_i: Dict[int, Set[str | int]] = {}\n",
    "        for r in dff[[\"id_batiment\",\"infra_id\"]].drop_duplicates().itertuples(index=False):\n",
    "            b_to_i.setdefault(int(r.id_batiment), set()).add(r.infra_id)\n",
    "            infras[r.infra_id].batiments.add(int(r.id_batiment))\n",
    "\n",
    "        # créer objets Building\n",
    "        buildings: Dict[int, Building] = {}\n",
    "        nb_map = dff.groupby(\"id_batiment\")[\"nb_maisons\"].max().to_dict()\n",
    "        for bid in sorted(b_to_i.keys()):\n",
    "            buildings[bid] = Building(id=bid, nb_maisons=nb_map.get(bid))\n",
    "\n",
    "        return cls(buildings=buildings, infras=infras, b_to_i=b_to_i)\n",
    "\n",
    "    # opérations\n",
    "    def repair_infras_of(self, bid: int) -> List[str | int]:\n",
    "        \"\"\"Marque comme réparées toutes les infras du bâtiment.\"\"\"\n",
    "        repaired = []\n",
    "        for iid in list(self.b_to_i.get(bid, set())):\n",
    "            if iid in self.unrepaired:\n",
    "                self.unrepaired.remove(iid)\n",
    "                repaired.append(iid)\n",
    "        return repaired\n",
    "\n",
    "    def building_order_plan(self) -> pd.DataFrame:\n",
    "        \"\"\"Algorithme itératif : commence par les plus faciles (somme des difficultés).\"\"\"\n",
    "        remaining = set(self.buildings.keys())\n",
    "        plan_rows = []\n",
    "        cum_cost = 0.0\n",
    "        step = 0\n",
    "\n",
    "        while remaining:\n",
    "            # tri grâce à __lt__ (difficulty asc, tie-break desc)\n",
    "            sorted_bs = sorted((self.buildings[b] for b in remaining))\n",
    "            best = sorted_bs[0]\n",
    "            step += 1\n",
    "            marginal = best.difficulty()\n",
    "            cum_cost += marginal\n",
    "            repaired = self.repair_infras_of(best.id)\n",
    "\n",
    "            plan_rows.append({\n",
    "                \"ordre\": step,\n",
    "                \"id_batiment\": best.id,\n",
    "                \"cout_marginal\": float(marginal),\n",
    "                \"cout_cumule\": float(cum_cost),\n",
    "                \"nb_infras_reparees\": len(repaired),\n",
    "                \"infras_reparees\": \";\".join(map(str, repaired)),\n",
    "                \"tie_break\": float(best.tie_break())\n",
    "            })\n",
    "            remaining.remove(best.id)\n",
    "\n",
    "        return pd.DataFrame(plan_rows)\n",
    "\n",
    "    # représentation\n",
    "    def __repr__(self):\n",
    "        return f\"Network(|B|={len(self.buildings)}, |I|={len(self.infras)})\"\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Exécution sur ton fichier\n",
    "# ===========================\n",
    "# 1) Charger les données\n",
    "#    -> adapte le chemin si besoin (ex: '/mnt/data/reseau_en_arbre_clean.csv')\n",
    "CSV = \"reseau_en_arbre_clean.csv\"\n",
    "df = pd.read_csv(CSV)\n",
    "\n",
    "# 2) Construire le réseau (tu peux ajuster α, β, γ)\n",
    "net = Network.from_dataframe(\n",
    "    df,\n",
    "    alpha_replace=1.2,   # pénalise plus les infras \"à remplacer\"\n",
    "    beta_complex=0.6,    # pénalise un peu plus \"complexe/difficile\"\n",
    "    gamma_simple=0.25    # bonus (réduction) si \"simple\"\n",
    ")\n",
    "\n",
    "print(net)  # __repr__\n",
    "\n",
    "# 3) Générer le plan (POO)\n",
    "plan = net.building_order_plan()\n",
    "\n",
    "# 4) Résultats\n",
    "print(\"\\n=== APERÇU PLAN (top 20) ===\")\n",
    "print(plan.head(20))\n",
    "print(\"\\nCOUT CUMULÉ FINAL:\", float(plan[\"cout_cumule\"].iloc[-1]))\n",
    "print(\"BÂTIMENTS COUVERTS:\", len(plan))\n",
    "\n",
    "# 5) Sauvegarde\n",
    "plan_path = \"/mnt/data/plan_raccordement_oo.csv\"\n",
    "plan.to_csv(plan_path, index=False)\n",
    "print(\"\\nExport:\", plan_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4eb1a63-8281-4513-b299-acb73c831d13",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "CSV introuvable: reseau_en_arbre.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 259\u001b[39m\n\u001b[32m    256\u001b[39m csv_path = \u001b[33m\"\u001b[39m\u001b[33mreseau_en_arbre.csv\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# adapte si besoin\u001b[39;00m\n\u001b[32m    258\u001b[39m analyzer = NetworkAnalyzer(csv_path)\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m analyzer.build_graph()\n\u001b[32m    261\u001b[39m summary = analyzer.compute_summary()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mNetworkAnalyzer.load_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> pd.DataFrame:\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.csv_path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCSV introuvable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.csv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m     df = pd.read_csv(\u001b[38;5;28mself\u001b[39m.csv_path)\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Vérif colonnes source/target\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: CSV introuvable: reseau_en_arbre.csv"
     ]
    }
   ],
   "source": [
    "# analyse_topologie.py\n",
    "# Requirements: pandas, networkx, matplotlib (optionnel pour la visualisation)\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class TopologySummary:\n",
    "    n_nodes: int\n",
    "    n_edges: int\n",
    "    is_connected: bool\n",
    "    n_components: int\n",
    "    component_sizes: List[int]\n",
    "    density: float\n",
    "    avg_degree: float\n",
    "    diameter_lcc: Optional[int]\n",
    "    aspl_lcc: Optional[float]\n",
    "    n_bridges: int\n",
    "    n_articulation_points: int\n",
    "    total_length: Optional[float]\n",
    "    total_length_to_replace: Optional[float]\n",
    "    total_length_intact: Optional[float]\n",
    "\n",
    "class NetworkAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyse préliminaire de la topologie du réseau.\n",
    "    Nœuds = bâtiments ; Arêtes = infrastructures.\n",
    "    Attributs d'arêtes gérés si présents: length|longueur, cost|cout, status, houses_served|nb_maisons.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path: str | Path):\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.edges_df: Optional[pd.DataFrame] = None\n",
    "        self.G: Optional[nx.Graph] = None\n",
    "\n",
    "    # -------- Chargement & normalisation --------\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        if not self.csv_path.exists():\n",
    "            raise FileNotFoundError(f\"CSV introuvable: {self.csv_path}\")\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "\n",
    "        # Vérif colonnes source/target\n",
    "        possible_src = [c for c in df.columns if c.lower() in {\"source\",\"src\",\"from\",\"u\"}]\n",
    "        possible_tgt = [c for c in df.columns if c.lower() in {\"target\",\"tgt\",\"to\",\"v\"}]\n",
    "        if not possible_src or not possible_tgt:\n",
    "            raise ValueError(\"Colonnes source/target non trouvées (attendues: source/target).\")\n",
    "\n",
    "        # Standardise les noms\n",
    "        df = df.rename(columns={\n",
    "            possible_src[0]: \"source\",\n",
    "            possible_tgt[0]: \"target\"\n",
    "        })\n",
    "\n",
    "        # Normalise quelques attributs d'arêtes courants (optionnels)\n",
    "        def pick(*names):\n",
    "            for n in names:\n",
    "                if n in df.columns: return n\n",
    "            return None\n",
    "\n",
    "        length_col = pick(\"length\",\"longueur\",\"len\",\"distance\",\"dist\",\"cost\",\"cout\")\n",
    "        status_col = pick(\"status\",\"etat\",\"state\")\n",
    "        houses_col = pick(\"houses_served\",\"nb_maisons\",\"houses\",\"bâtiments desservis\",\"batiments_servis\",\"served\")\n",
    "\n",
    "        # Valeurs par défaut\n",
    "        if length_col is None:\n",
    "            df[\"length\"] = 1.0\n",
    "            length_col = \"length\"\n",
    "        # status par défaut = \"to_replace\"\n",
    "        if status_col is None:\n",
    "            df[\"status\"] = \"to_replace\"\n",
    "            status_col = \"status\"\n",
    "\n",
    "        # Nettoyage léger\n",
    "        df[\"source\"] = df[\"source\"].astype(str)\n",
    "        df[\"target\"] = df[\"target\"].astype(str)\n",
    "        df[length_col] = pd.to_numeric(df[length_col], errors=\"coerce\").fillna(1.0)\n",
    "        df[status_col] = df[status_col].astype(str).str.lower().str.strip()\n",
    "        if houses_col:\n",
    "            df[houses_col] = pd.to_numeric(df[houses_col], errors=\"coerce\").fillna(1).astype(int)\n",
    "\n",
    "        # Conserve noms standardisés pour la suite\n",
    "        self._length_col = length_col\n",
    "        self._status_col = status_col\n",
    "        self._houses_col = houses_col\n",
    "        self.edges_df = df\n",
    "        return df\n",
    "\n",
    "    # -------- Construction du graphe --------\n",
    "    def build_graph(self) -> nx.Graph:\n",
    "        if self.edges_df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        G = nx.Graph()\n",
    "        attrs = [\"source\",\"target\", self._length_col, self._status_col]\n",
    "        if self._houses_col:\n",
    "            attrs.append(self._houses_col)\n",
    "\n",
    "        for _, row in self.edges_df.iterrows():\n",
    "            u, v = row[\"source\"], row[\"target\"]\n",
    "            data = {\n",
    "                \"length\": float(row[self._length_col]),\n",
    "                \"status\": row[self._status_col],\n",
    "            }\n",
    "            if self._houses_col:\n",
    "                data[\"houses_served\"] = int(row[self._houses_col])\n",
    "            # Ajout arête\n",
    "            G.add_edge(u, v, **data)\n",
    "\n",
    "        self.G = G\n",
    "        return G\n",
    "\n",
    "    # -------- Métriques de topologie --------\n",
    "    def compute_summary(self) -> TopologySummary:\n",
    "        if self.G is None:\n",
    "            self.build_graph()\n",
    "        G = self.G\n",
    "\n",
    "        n_nodes = G.number_of_nodes()\n",
    "        n_edges = G.number_of_edges()\n",
    "        is_connected = nx.is_connected(G) if n_nodes > 0 else False\n",
    "\n",
    "        components = [len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n",
    "        n_components = len(components)\n",
    "        component_sizes = components[:10]  # top 10 tailles\n",
    "\n",
    "        density = nx.density(G) if n_nodes > 1 else 0.0\n",
    "        avg_degree = (2*n_edges / n_nodes) if n_nodes > 0 else 0.0\n",
    "\n",
    "        # LCC = plus grande composante\n",
    "        diameter_lcc = None\n",
    "        aspl_lcc = None\n",
    "        if n_nodes > 0 and n_components >= 1:\n",
    "            lcc_nodes = max(nx.connected_components(G), key=len)\n",
    "            H = G.subgraph(lcc_nodes).copy()\n",
    "            if H.number_of_nodes() > 1:\n",
    "                # Diamètre et longueur moyenne des plus courts chemins\n",
    "                try:\n",
    "                    diameter_lcc = nx.diameter(H)\n",
    "                except nx.NetworkXError:\n",
    "                    diameter_lcc = None\n",
    "                try:\n",
    "                    aspl_lcc = nx.average_shortest_path_length(H, weight=None)\n",
    "                except nx.NetworkXError:\n",
    "                    aspl_lcc = None\n",
    "\n",
    "        # Ponts & points d'articulation\n",
    "        bridges = list(nx.bridges(G)) if n_edges > 0 else []\n",
    "        apoints = list(nx.articulation_points(G)) if n_nodes > 0 else []\n",
    "\n",
    "        # Stats d'attributs (si length/status)\n",
    "        total_length = total_length_to_replace = total_length_intact = None\n",
    "        if n_edges > 0:\n",
    "            lengths = nx.get_edge_attributes(G, \"length\")\n",
    "            statuses = nx.get_edge_attributes(G, \"status\")\n",
    "            total_length = float(sum(lengths.values()))\n",
    "            total_length_to_replace = float(sum(l for (e, l) in lengths.items() \n",
    "                                                if statuses.get(e, \"to_replace\") == \"to_replace\"))\n",
    "            total_length_intact = total_length - total_length_to_replace\n",
    "\n",
    "        return TopologySummary(\n",
    "            n_nodes=n_nodes,\n",
    "            n_edges=n_edges,\n",
    "            is_connected=is_connected,\n",
    "            n_components=n_components,\n",
    "            component_sizes=component_sizes,\n",
    "            density=density,\n",
    "            avg_degree=avg_degree,\n",
    "            diameter_lcc=diameter_lcc,\n",
    "            aspl_lcc=aspl_lcc,\n",
    "            n_bridges=len(bridges),\n",
    "            n_articulation_points=len(apoints),\n",
    "            total_length=total_length,\n",
    "            total_length_to_replace=total_length_to_replace,\n",
    "            total_length_intact=total_length_intact,\n",
    "        )\n",
    "\n",
    "    # -------- Rapports CSV optionnels --------\n",
    "    def export_reports(self, out_dir: str | Path = \"outputs\") -> Dict[str, Path]:\n",
    "        \"\"\"\n",
    "        Exporte: \n",
    "          - components.csv : composantes et leurs tailles\n",
    "          - degree_stats.csv : degré des nœuds (degree, is_articulation)\n",
    "          - bridges.csv : liste des ponts\n",
    "        \"\"\"\n",
    "        if self.G is None:\n",
    "            self.build_graph()\n",
    "        G = self.G\n",
    "        out_dir = Path(out_dir)\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        artifacts: Dict[str, Path] = {}\n",
    "\n",
    "        # Composantes\n",
    "        comps = []\n",
    "        for i, comp in enumerate(sorted(nx.connected_components(G), key=len, reverse=True), start=1):\n",
    "            for n in comp:\n",
    "                comps.append({\"component_id\": i, \"node\": n, \"size\": len(comp)})\n",
    "        df_comps = pd.DataFrame(comps).sort_values([\"component_id\",\"node\"])\n",
    "        p1 = out_dir / \"components.csv\"\n",
    "        df_comps.to_csv(p1, index=False)\n",
    "        artifacts[\"components\"] = p1\n",
    "\n",
    "        # Degrés & articulation\n",
    "        deg = dict(G.degree())\n",
    "        ap = set(nx.articulation_points(G)) if G.number_of_nodes() > 0 else set()\n",
    "        df_deg = pd.DataFrame(\n",
    "            [{\"node\": n, \"degree\": d, \"is_articulation\": n in ap} for n, d in deg.items()]\n",
    "        ).sort_values([\"degree\",\"node\"], ascending=[False, True])\n",
    "        p2 = out_dir / \"degree_stats.csv\"\n",
    "        df_deg.to_csv(p2, index=False)\n",
    "        artifacts[\"degree_stats\"] = p2\n",
    "\n",
    "        # Ponts\n",
    "        bridges = list(nx.bridges(G)) if G.number_of_edges() > 0 else []\n",
    "        df_br = pd.DataFrame([{\"u\": u, \"v\": v} for u, v in bridges])\n",
    "        p3 = out_dir / \"bridges.csv\"\n",
    "        df_br.to_csv(p3, index=False)\n",
    "        artifacts[\"bridges\"] = p3\n",
    "\n",
    "        return artifacts\n",
    "\n",
    "    # -------- Visualisation rapide (optionnelle) --------\n",
    "    def quick_plot(self, figsize: Tuple[int,int]=(10,8)) -> None:\n",
    "        \"\"\"\n",
    "        Aperçu du graphe avec spring_layout. \n",
    "        Les arêtes 'to_replace' sont plus épaisses (pour les repérer facilement).\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        if self.G is None:\n",
    "            self.build_graph()\n",
    "        G = self.G\n",
    "\n",
    "        pos = nx.spring_layout(G, seed=42)  # layout générique (pas géographique)\n",
    "\n",
    "        # Sépare arêtes par status\n",
    "        ed_intact = [(u,v) for u,v,d in G.edges(data=True) if d.get(\"status\",\"to_replace\") != \"to_replace\"]\n",
    "        ed_replace = [(u,v) for u,v,d in G.edges(data=True) if d.get(\"status\",\"to_replace\") == \"to_replace\"]\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=60)\n",
    "        # On ne fixe pas de couleurs personnalisées (simples traits)\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=ed_intact, width=1.0)\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=ed_replace, width=2.0, style=\"dashed\")\n",
    "        nx.draw_networkx_labels(G, pos, font_size=7)\n",
    "        plt.title(\"Aperçu topologique — continu: intact / pointillé: à remplacer\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ------------- Exemple d'utilisation -------------\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"reseau_en_arbre.csv\"  # adapte si besoin\n",
    "\n",
    "    analyzer = NetworkAnalyzer(csv_path)\n",
    "    analyzer.load_data()\n",
    "    analyzer.build_graph()\n",
    "    summary = analyzer.compute_summary()\n",
    "\n",
    "    print(\"\\n=== RÉSUMÉ TOPOLOGIQUE ===\")\n",
    "    print(f\"Nodes: {summary.n_nodes}\")\n",
    "    print(f\"Edges: {summary.n_edges}\")\n",
    "    print(f\"Connected: {summary.is_connected}\")\n",
    "    print(f\"Components: {summary.n_components}  (top sizes: {summary.component_sizes})\")\n",
    "    print(f\"Density: {summary.density:.6f}   AvgDegree: {summary.avg_degree:.3f}\")\n",
    "    print(f\"Diameter (LCC): {summary.diameter_lcc}   ASPL (LCC): {summary.aspl_lcc}\")\n",
    "    print(f\"Bridges: {summary.n_bridges}   Articulation points: {summary.n_articulation_points}\")\n",
    "    if summary.total_length is not None:\n",
    "        print(f\"Total length: {summary.total_length:.2f}\")\n",
    "        print(f\"  - to_replace: {summary.total_length_to_replace:.2f}\")\n",
    "        print(f\"  - intact    : {summary.total_length_intact:.2f}\")\n",
    "\n",
    "    #Exports (décommente si tu veux des CSV)\n",
    "    artifacts = analyzer.export_reports(\"outputs\")\n",
    "    print(\"CSV écrits:\", artifacts)\n",
    "\n",
    "    #Visualisation (décommente pour un aperçu)\n",
    "    analyzer.quick_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a0da6-234c-49aa-abab-0a2d3ecb8d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pandas as pd, networkx as nx\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "# ---------- Config fichiers ----------\n",
    "BASE = Path(\".\")\n",
    "FP_BATS = BASE / \"batiments.csv\"\n",
    "FP_INFRA = BASE / \"infra.csv\"\n",
    "FP_LINKS = BASE / \"reseau_en_arbre.csv\"   # <<--- fournis ceci\n",
    "\n",
    "OUT = BASE / \"outputs\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Barèmes (annexe.docx) ----------\n",
    "PRICE_PER_M = {\"aerien\": 500.0, \"semi-aerien\": 750.0, \"fourreau\": 900.0}\n",
    "DUR_PER_M_H = {\"aerien\": 2.0,   \"semi-aerien\": 4.0,   \"fourreau\": 5.0}\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand in low:\n",
    "            return low[cand]\n",
    "    # tolérance accents/ponctuation\n",
    "    import unicodedata, re\n",
    "    norm = {re.sub(r\"[^a-z0-9]\",\"\",unicodedata.normalize(\"NFD\",c).encode(\"ascii\",\"ignore\").decode().lower()): c\n",
    "            for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        k = re.sub(r\"[^a-z0-9]\",\"\",cand.lower())\n",
    "        if k in norm: return norm[k]\n",
    "    return None\n",
    "\n",
    "def normalize_type(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"aerien\"\n",
    "    import unicodedata\n",
    "    s2 = unicodedata.normalize(\"NFD\", s.strip().lower()).encode(\"ascii\",\"ignore\").decode()\n",
    "    if \"semi\" in s2: return \"semi-aerien\"\n",
    "    if \"fourr\" in s2 or \"gaine\" in s2 or \"conduit\" in s2: return \"fourreau\"\n",
    "    return \"aerien\"\n",
    "\n",
    "@dataclass\n",
    "class TopologySummary:\n",
    "    n_nodes: int\n",
    "    n_edges: int\n",
    "    is_connected: bool\n",
    "    n_components: int\n",
    "    component_sizes: List[int]\n",
    "    density: float\n",
    "    avg_degree: float\n",
    "    diameter_lcc: Optional[int]\n",
    "    aspl_lcc: Optional[float]\n",
    "    n_bridges: int\n",
    "    n_articulation_points: int\n",
    "    total_length: float\n",
    "    total_cost_to_replace: float\n",
    "    total_duration_h: float\n",
    "\n",
    "class Analyzer:\n",
    "    def __init__(self, fp_bats: Path, fp_infra: Path, fp_links: Path):\n",
    "        self.fp_bats, self.fp_infra, self.fp_links = fp_bats, fp_infra, fp_links\n",
    "        self.bats = pd.read_csv(fp_bats)\n",
    "        self.infra = pd.read_csv(fp_infra)\n",
    "        self.links = pd.read_csv(fp_links)\n",
    "        self.G: Optional[nx.Graph] = None\n",
    "        self._prepare_tables()\n",
    "\n",
    "    def _prepare_tables(self):\n",
    "        # infra.csv: id_infra, type_infra\n",
    "        id_infra = pick_col(self.infra, [\"id_infra\",\"infra_id\",\"id\"])\n",
    "        type_infra = pick_col(self.infra, [\"type_infra\",\"type\",\"categorie\",\"pose\",\"mode\"])\n",
    "        if not id_infra: raise ValueError(\"infra.csv doit contenir id_infra\")\n",
    "        if not type_infra: raise ValueError(\"infra.csv doit contenir type_infra\")\n",
    "        self.infra = self.infra.rename(columns={id_infra:\"id_infra\", type_infra:\"type_infra\"})\n",
    "        self.infra[\"type_infra\"] = self.infra[\"type_infra\"].apply(normalize_type)\n",
    "\n",
    "        # links: source, target, id_infra, length, status\n",
    "        src = pick_col(self.links, [\"source\",\"from\",\"u\",\"src\"])\n",
    "        tgt = pick_col(self.links, [\"target\",\"to\",\"v\",\"tgt\",\"dest\"])\n",
    "        lin = pick_col(self.links, [\"length\",\"longueur\",\"len\",\"distance\",\"dist\",\"metres\",\"m\"])\n",
    "        status = pick_col(self.links, [\"status\",\"etat\",\"state\",\"a_remplacer\",\"to_replace_flag\"])\n",
    "        link_infra = pick_col(self.links, [\"id_infra\",\"infra_id\",\"id\"])\n",
    "        if not (src and tgt and link_infra):\n",
    "            raise ValueError(\"La table des liaisons doit contenir au minimum: source, target, id_infra\")\n",
    "        if not lin:\n",
    "            self.links[\"length\"] = 1.0; lin = \"length\"\n",
    "        if not status:\n",
    "            self.links[\"status\"] = \"to_replace\"; status = \"status\"\n",
    "\n",
    "        self.links = self.links.rename(columns={src:\"source\", tgt:\"target\", lin:\"length\", status:\"status\", link_infra:\"id_infra\"})\n",
    "        self.links[\"source\"] = self.links[\"source\"].astype(str)\n",
    "        self.links[\"target\"] = self.links[\"target\"].astype(str)\n",
    "        self.links[\"id_infra\"] = self.links[\"id_infra\"].astype(str)\n",
    "        self.links[\"length\"] = pd.to_numeric(self.links[\"length\"], errors=\"coerce\").fillna(1.0)\n",
    "        self.links[\"status\"] = self.links[\"status\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "        # join pour obtenir le type\n",
    "        self.links = self.links.merge(self.infra, on=\"id_infra\", how=\"left\")\n",
    "        self.links[\"type_infra\"] = self.links[\"type_infra\"].fillna(\"aerien\").apply(normalize_type)\n",
    "\n",
    "        # coûts/durées par arête si à remplacer\n",
    "        self.links[\"edge_cost\"] = self.links.apply(\n",
    "            lambda r: PRICE_PER_M.get(r[\"type_infra\"], 500.0) * float(r[\"length\"]) if r[\"status\"] in (\"to_replace\",\"1\",\"true\",\"yes\",\"y\") else 0.0,\n",
    "            axis=1\n",
    "        )\n",
    "        self.links[\"edge_duration_h\"] = self.links.apply(\n",
    "            lambda r: DUR_PER_M_H.get(r[\"type_infra\"], 2.0) * float(r[\"length\"]) if r[\"status\"] in (\"to_replace\",\"1\",\"true\",\"yes\",\"y\") else 0.0,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    def build_graph(self) -> nx.Graph:\n",
    "        G = nx.Graph()\n",
    "        for _, r in self.links.iterrows():\n",
    "            G.add_edge(\n",
    "                r[\"source\"], r[\"target\"],\n",
    "                id_infra=r[\"id_infra\"],\n",
    "                length=float(r[\"length\"]),\n",
    "                status=r[\"status\"],\n",
    "                type=r[\"type_infra\"],\n",
    "                edge_cost=float(r[\"edge_cost\"]),\n",
    "                edge_duration_h=float(r[\"edge_duration_h\"]),\n",
    "            )\n",
    "        # ajoute les bâtiments isolés éventuels\n",
    "        id_b = pick_col(self.bats, [\"id_batiment\",\"building_id\",\"id\",\"node\"])\n",
    "        if id_b:\n",
    "            for b in self.bats[id_b].astype(str).unique():\n",
    "                if b not in G: G.add_node(b)\n",
    "        self.G = G\n",
    "        return G\n",
    "\n",
    "    def summary(self) -> TopologySummary:\n",
    "        if self.G is None: self.build_graph()\n",
    "        G = self.G\n",
    "        n_nodes = G.number_of_nodes()\n",
    "        n_edges = G.number_of_edges()\n",
    "        is_conn = nx.is_connected(G) if n_nodes > 0 else False\n",
    "        comps = [len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n",
    "        dens = nx.density(G) if n_nodes > 1 else 0.0\n",
    "        avg_deg = (2*n_edges / n_nodes) if n_nodes > 0 else 0.0\n",
    "\n",
    "        # LCC\n",
    "        diam = aspl = None\n",
    "        if n_nodes > 1 and comps:\n",
    "            lcc = max(nx.connected_components(G), key=len)\n",
    "            H = G.subgraph(lcc).copy()\n",
    "            if H.number_of_nodes() > 1 and H.number_of_edges() > 0:\n",
    "                try: diam = nx.diameter(H)\n",
    "                except: pass\n",
    "                try: aspl = nx.average_shortest_path_length(H)\n",
    "                except: pass\n",
    "\n",
    "        bridges = list(nx.bridges(G)) if n_edges > 0 else []\n",
    "        arts = list(nx.articulation_points(G)) if n_nodes > 0 else []\n",
    "        lengths = nx.get_edge_attributes(G, \"length\")\n",
    "        total_length = float(sum(lengths.values())) if lengths else 0.0\n",
    "        total_cost = float(sum(nx.get_edge_attributes(G, \"edge_cost\").values() or [0.0]))\n",
    "        total_dur = float(sum(nx.get_edge_attributes(G, \"edge_duration_h\").values() or [0.0]))\n",
    "\n",
    "        return TopologySummary(\n",
    "            n_nodes, n_edges, is_conn, len(comps), comps[:10], dens, avg_deg,\n",
    "            diam, aspl, len(bridges), len(arts), total_length, total_cost, total_dur\n",
    "        )\n",
    "\n",
    "    def export_reports(self):\n",
    "        if self.G is None: self.build_graph()\n",
    "        G = self.G\n",
    "        # composantes\n",
    "        comp_rows = []\n",
    "        for i, comp in enumerate(sorted(nx.connected_components(G), key=len, reverse=True), start=1):\n",
    "            for n in comp:\n",
    "                comp_rows.append({\"component_id\": i, \"node\": n, \"size\": len(comp)})\n",
    "        pd.DataFrame(comp_rows).to_csv(OUT / \"components.csv\", index=False)\n",
    "        # degrés + points d’articulation\n",
    "        deg = dict(G.degree())\n",
    "        arts = set(nx.articulation_points(G)) if G.number_of_nodes() > 0 else set()\n",
    "        pd.DataFrame([{\"node\": n, \"degree\": d, \"is_articulation\": n in arts} for n,d in deg.items()])\\\n",
    "          .sort_values([\"degree\",\"node\"], ascending=[False, True])\\\n",
    "          .to_csv(OUT / \"degree_stats.csv\", index=False)\n",
    "        # ponts\n",
    "        pd.DataFrame([{\"u\":u,\"v\":v} for (u,v) in (list(nx.bridges(G)) if G.number_of_edges()>0 else [])])\\\n",
    "          .to_csv(OUT / \"bridges.csv\", index=False)\n",
    "        # arêtes enrichies\n",
    "        self.links.to_csv(OUT / \"edges_with_costs.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = Analyzer(FP_BATS, FP_INFRA, FP_LINKS)\n",
    "    analyzer.build_graph()\n",
    "    s = analyzer.summary()\n",
    "    print(\"=== RÉSUMÉ TOPOLOGIQUE ===\")\n",
    "    print(f\"Nodes: {s.n_nodes}  Edges: {s.n_edges}  Connected: {s.is_connected}\")\n",
    "    print(f\"Components: {s.n_components} (top sizes: {s.component_sizes})\")\n",
    "    print(f\"Density: {s.density:.6f}   AvgDegree: {s.avg_degree:.3f}\")\n",
    "    print(f\"Diameter (LCC): {s.diameter_lcc}   ASPL (LCC): {s.aspl_lcc}\")\n",
    "    print(f\"Bridges: {s.n_bridges}   Articulation points: {s.n_articulation_points}\")\n",
    "    print(f\"Total length: {s.total_length:.2f} m\")\n",
    "    print(f\"Total COST to_replace: {s.total_cost_to_replace:,.2f} €\")\n",
    "    print(f\"Total DURATION to_replace: {s.total_duration_h:,.2f} h\")\n",
    "    analyzer.export_reports()\n",
    "    print(f\"\\nRapports dans: {OUT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25dcca39-bacf-4f2a-9852-ad66f7b4c6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline terminé.\n",
      "KPIs: {'cout_total_proxy': 2652.819, 'prises_total': 389, 'prises_reconnectees': 389, 'taux_reconnexion': 100.0, 'longueur_reparee_totale': 2652.82, 'nb_etapes': 380}\n",
      "Exports :\n",
      " - outputs_raccordement\\ordre_de_raccordement.csv\n",
      " - outputs_raccordement\\travaux_par_etape.csv\n",
      " - outputs_raccordement\\kpis.json\n",
      " - outputs_raccordement\\planification_raccordement.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rang</th>\n",
       "      <th>batiment</th>\n",
       "      <th>cout_incremental</th>\n",
       "      <th>cout_cumule</th>\n",
       "      <th>prises_cumulees</th>\n",
       "      <th>aretes_reparees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>B:E000228</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>B:E000229</td>\n",
       "      <td>5.609</td>\n",
       "      <td>5.609</td>\n",
       "      <td>2</td>\n",
       "      <td>[(B:E000228, I:P008012), (I:P008012, B:E000229)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>B:E000260</td>\n",
       "      <td>2.804</td>\n",
       "      <td>8.413</td>\n",
       "      <td>3</td>\n",
       "      <td>[(I:P008012, B:E000260)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>B:E000275</td>\n",
       "      <td>2.804</td>\n",
       "      <td>11.217</td>\n",
       "      <td>4</td>\n",
       "      <td>[(I:P008012, B:E000275)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>B:E000303</td>\n",
       "      <td>2.804</td>\n",
       "      <td>14.022</td>\n",
       "      <td>5</td>\n",
       "      <td>[(I:P008012, B:E000303)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>B:E000317</td>\n",
       "      <td>2.804</td>\n",
       "      <td>16.826</td>\n",
       "      <td>6</td>\n",
       "      <td>[(I:P008012, B:E000317)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>B:E000327</td>\n",
       "      <td>2.804</td>\n",
       "      <td>19.630</td>\n",
       "      <td>7</td>\n",
       "      <td>[(I:P008012, B:E000327)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>B:E000358</td>\n",
       "      <td>2.804</td>\n",
       "      <td>22.435</td>\n",
       "      <td>8</td>\n",
       "      <td>[(I:P008012, B:E000358)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>B:E000363</td>\n",
       "      <td>2.804</td>\n",
       "      <td>25.239</td>\n",
       "      <td>9</td>\n",
       "      <td>[(I:P008012, B:E000363)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rang   batiment  cout_incremental  cout_cumule  prises_cumulees  \\\n",
       "0     0  B:E000228             0.000        0.000                1   \n",
       "1     0       None             0.000        0.000                1   \n",
       "2     1  B:E000229             5.609        5.609                2   \n",
       "3     2  B:E000260             2.804        8.413                3   \n",
       "4     3  B:E000275             2.804       11.217                4   \n",
       "5     4  B:E000303             2.804       14.022                5   \n",
       "6     5  B:E000317             2.804       16.826                6   \n",
       "7     6  B:E000327             2.804       19.630                7   \n",
       "8     7  B:E000358             2.804       22.435                8   \n",
       "9     8  B:E000363             2.804       25.239                9   \n",
       "\n",
       "                                    aretes_reparees  \n",
       "0                                                []  \n",
       "1                                                []  \n",
       "2  [(B:E000228, I:P008012), (I:P008012, B:E000229)]  \n",
       "3                          [(I:P008012, B:E000260)]  \n",
       "4                          [(I:P008012, B:E000275)]  \n",
       "5                          [(I:P008012, B:E000303)]  \n",
       "6                          [(I:P008012, B:E000317)]  \n",
       "7                          [(I:P008012, B:E000327)]  \n",
       "8                          [(I:P008012, B:E000358)]  \n",
       "9                          [(I:P008012, B:E000363)]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>etape</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>longueur</th>\n",
       "      <th>cout_proxy</th>\n",
       "      <th>etat_initial</th>\n",
       "      <th>type_intervention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B:E000228</td>\n",
       "      <td>I:P008012</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>intacte</td>\n",
       "      <td>pose/reutilisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I:P008012</td>\n",
       "      <td>B:E000229</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>intacte</td>\n",
       "      <td>pose/reutilisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I:P008012</td>\n",
       "      <td>B:E000260</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>intacte</td>\n",
       "      <td>pose/reutilisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I:P008012</td>\n",
       "      <td>B:E000275</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>intacte</td>\n",
       "      <td>pose/reutilisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I:P008012</td>\n",
       "      <td>B:E000303</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>intacte</td>\n",
       "      <td>pose/reutilisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>I:P008012</td>\n",
       "      <td>B:E000317</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>intacte</td>\n",
       "      <td>pose/reutilisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>I:P008012</td>\n",
       "      <td>B:E000327</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>intacte</td>\n",
       "      <td>pose/reutilisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>I:P008012</td>\n",
       "      <td>B:E000358</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>intacte</td>\n",
       "      <td>pose/reutilisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>I:P008012</td>\n",
       "      <td>B:E000363</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>intacte</td>\n",
       "      <td>pose/reutilisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>I:P008012</td>\n",
       "      <td>B:E000370</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>2.804326</td>\n",
       "      <td>intacte</td>\n",
       "      <td>pose/reutilisation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   etape          u          v  longueur  cout_proxy etat_initial  \\\n",
       "0      1  B:E000228  I:P008012  2.804326    2.804326      intacte   \n",
       "1      1  I:P008012  B:E000229  2.804326    2.804326      intacte   \n",
       "2      2  I:P008012  B:E000260  2.804326    2.804326      intacte   \n",
       "3      3  I:P008012  B:E000275  2.804326    2.804326      intacte   \n",
       "4      4  I:P008012  B:E000303  2.804326    2.804326      intacte   \n",
       "5      5  I:P008012  B:E000317  2.804326    2.804326      intacte   \n",
       "6      6  I:P008012  B:E000327  2.804326    2.804326      intacte   \n",
       "7      7  I:P008012  B:E000358  2.804326    2.804326      intacte   \n",
       "8      8  I:P008012  B:E000363  2.804326    2.804326      intacte   \n",
       "9      9  I:P008012  B:E000370  2.804326    2.804326      intacte   \n",
       "\n",
       "    type_intervention  \n",
       "0  pose/reutilisation  \n",
       "1  pose/reutilisation  \n",
       "2  pose/reutilisation  \n",
       "3  pose/reutilisation  \n",
       "4  pose/reutilisation  \n",
       "5  pose/reutilisation  \n",
       "6  pose/reutilisation  \n",
       "7  pose/reutilisation  \n",
       "8  pose/reutilisation  \n",
       "9  pose/reutilisation  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json, os\n",
    "\n",
    "# ---------------------------\n",
    "# PARAMÈTRES RAPIDES (à adapter)\n",
    "# ---------------------------\n",
    "PATH_XLSX = \"reseau_en_arbre.xlsx\" \n",
    "SHEET_NAME = 0                                \n",
    "OUT_DIR = \"outputs_raccordement\"    \n",
    "\n",
    "# Pondérations / coût\n",
    "ALPHA = 1.0           \n",
    "COUT_UNITAIRE = 1.0    \n",
    "DELTA_REMPLACEMENT = 0.5  \n",
    "BUDGET = None          \n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_ORDRE = os.path.join(OUT_DIR, \"ordre_de_raccordement.csv\")\n",
    "OUT_TRAVAUX = os.path.join(OUT_DIR, \"travaux_par_etape.csv\")\n",
    "OUT_KPIS = os.path.join(OUT_DIR, \"kpis.json\")\n",
    "OUT_XLSX = os.path.join(OUT_DIR, \"planification_raccordement.xlsx\")\n",
    "\n",
    "--\n",
    "def std_colnames(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.rename(columns={c: str(c).strip().lower().replace(\" \", \"_\") for c in df.columns})\n",
    "\n",
    "def normalize_etat(val) -> str:\n",
    "    if pd.isna(val): return \"intacte\"\n",
    "    s = str(val).strip().lower().replace(\"à\", \"a\")\n",
    "    if \"remplac\" in s or \"a_remplacer\" in s: return \"a_remplacer\"\n",
    "    if \"intact\" in s or \"ok\" in s or \"neuf\" in s: return \"intacte\"\n",
    "    return s\n",
    "\n",
    "# ---------------------------\n",
    "# Lecture du fichier\n",
    "# ---------------------------\n",
    "def read_input(path, sheet_name=0) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_excel(path, sheet_name=sheet_name)\n",
    "    except Exception:\n",
    "        # fallback CSV si jamais\n",
    "        df = pd.read_csv(path)\n",
    "    return std_colnames(df)\n",
    "\n",
    "edges_raw = read_input(PATH_XLSX, SHEET_NAME)\n",
    "\n",
    "\n",
    "has_edge_list = {\"source\", \"target\"}.issubset(edges_raw.columns)\n",
    "has_bipartite = {\"id_batiment\", \"infra_id\"}.issubset(edges_raw.columns)\n",
    "\n",
    "if not (has_edge_list or has_bipartite):\n",
    "    raise ValueError(\n",
    "        f\"Colonnes détectées: {list(edges_raw.columns)}\\n\"\n",
    "        \"Le fichier doit suivre l'un des schémas :\\n\"\n",
    "        \"- Edge list: source, target [, longueur, cout, etat, nb_maisons]\\n\"\n",
    "        \"- Biparti: id_batiment, infra_id [, infra_type, longueur, nb_maisons, etat]\"\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InfrastructureEL:\n",
    "    eid: int\n",
    "    u: str\n",
    "    v: str\n",
    "    longueur: float\n",
    "    cout: float\n",
    "    etat: str\n",
    "    nb_maisons: int\n",
    "    type_intervention: str\n",
    "    repaired: bool = False\n",
    "\n",
    "    def difficulte(self, alpha=ALPHA, delta_remplacement=DELTA_REMPLACEMENT) -> float:\n",
    "        cout_eff = self.cout * (1 + (delta_remplacement if self.etat == \"a_remplacer\" else 0.0))\n",
    "        denom = max(1, self.nb_maisons)\n",
    "        mix = alpha * cout_eff + (1 - alpha) * (self.longueur / denom)\n",
    "        return 0.0 if self.repaired else float(mix)\n",
    "\n",
    "@dataclass\n",
    "class BatimentEL:\n",
    "    bid: str\n",
    "    infras: List[int] = field(default_factory=list)\n",
    "\n",
    "class ReseauEL:\n",
    "    def __init__(self):\n",
    "        self.G = nx.Graph()\n",
    "        self.infras: Dict[int, InfrastructureEL] = {}\n",
    "        self.batiments: Dict[str, BatimentEL] = {}\n",
    "        self.repaired_edges: Set[int] = set()\n",
    "\n",
    "    def add_infra(self, infra: InfrastructureEL):\n",
    "        self.infras[infra.eid] = infra\n",
    "        self.G.add_edge(infra.u, infra.v, eid=infra.eid)\n",
    "        for node in (infra.u, infra.v):\n",
    "            if node not in self.batiments:\n",
    "                self.batiments[node] = BatimentEL(node, [])\n",
    "            self.batiments[node].infras.append(infra.eid)\n",
    "\n",
    "    def set_repaired(self, eid: int):\n",
    "        self.infras[eid].repaired = True\n",
    "        self.repaired_edges.add(eid)\n",
    "\n",
    "    def edge_weight(self, eid: int) -> float:\n",
    "        return self.infras[eid].difficulte()\n",
    "\n",
    "    def path_incremental_cost(self, path_nodes: List[str]) -> Tuple[float, List[int]]:\n",
    "        total, used_eids = 0.0, []\n",
    "        for u, v in zip(path_nodes[:-1], path_nodes[1:]):\n",
    "            eid = self.G[u][v][\"eid\"]\n",
    "            w = self.edge_weight(eid)\n",
    "            if w > 0:\n",
    "                total += w\n",
    "                used_eids.append(eid)\n",
    "        return total, used_eids\n",
    "\n",
    "    def multi_source_shortest_path(self, sources: Set[str]) -> Dict[str, Tuple[float, List[str]]]:\n",
    "        H = nx.Graph()\n",
    "        for u, v, d in self.G.edges(data=True):\n",
    "            H.add_edge(u, v, weight=self.edge_weight(d[\"eid\"]))\n",
    "        SUP = \"__SUP__\"\n",
    "        for s in sources:\n",
    "            H.add_edge(SUP, s, weight=0.0)\n",
    "        dist, paths = nx.single_source_dijkstra(H, SUP, weight=\"weight\")\n",
    "        dist.pop(SUP, None); paths.pop(SUP, None)\n",
    "        out = {}\n",
    "        for node, p in paths.items():\n",
    "            if p and p[0] == SUP: p = p[1:]\n",
    "            cost, used_eids = self.path_incremental_cost(p)\n",
    "            out[node] = (cost, p)\n",
    "        return out\n",
    "\n",
    "def run_pipeline_edge_list(edges: pd.DataFrame):\n",
    "    # Colonnes optionnelles\n",
    "    for new_name, candidates, default in [\n",
    "        (\"longueur\", [\"longueur\",\"length\",\"len_m\",\"distance\"], np.nan),\n",
    "        (\"cout\", [\"cout\",\"cost\",\"prix\",\"price\"], np.nan),\n",
    "        (\"etat\", [\"etat\",\"state\",\"condition\"], \"intacte\"),\n",
    "        (\"nb_maisons\", [\"nb_maisons\",\"maisons\",\"prises\",\"nb_prises\"], np.nan),\n",
    "        (\"type_intervention\", [\"type_intervention\",\"intervention\"], np.nan),\n",
    "    ]:\n",
    "        if new_name not in edges.columns:\n",
    "            for c in candidates:\n",
    "                if c in edges.columns:\n",
    "                    edges[new_name] = edges[c]; break\n",
    "            if new_name not in edges.columns:\n",
    "                edges[new_name] = default\n",
    "\n",
    "    edges[\"etat\"] = edges[\"etat\"].map(normalize_etat)\n",
    "    edges[\"nb_maisons\"] = edges[\"nb_maisons\"].fillna(1).astype(int)\n",
    "    if edges[\"cout\"].isna().all() and edges[\"longueur\"].isna().all():\n",
    "        edges[\"longueur\"] = 1.0\n",
    "    edges[\"cout\"] = edges[\"cout\"].fillna(0.0)\n",
    "    mask = edges[\"cout\"] == 0.0\n",
    "    edges.loc[mask, \"cout\"] = edges.loc[mask, \"longueur\"].fillna(0.0) * COUT_UNITAIRE\n",
    "    edges[\"longueur\"] = edges[\"longueur\"].fillna(0.0)\n",
    "    edges[\"type_intervention\"] = edges[\"type_intervention\"].fillna(\n",
    "        edges[\"etat\"].map(lambda s: \"remplacement\" if s == \"a_remplacer\" else \"pose/reutilisation\")\n",
    "    )\n",
    "\n",
    "    reseau = ReseauEL()\n",
    "    for i, row in edges.reset_index(drop=True).iterrows():\n",
    "        inf = InfrastructureEL(\n",
    "            eid=int(i),\n",
    "            u=str(row[\"source\"]), v=str(row[\"target\"]),\n",
    "            longueur=float(row[\"longueur\"] or 0.0),\n",
    "            cout=float(row[\"cout\"] or 0.0),\n",
    "            etat=normalize_etat(row[\"etat\"]),\n",
    "            nb_maisons=int(row[\"nb_maisons\"] or 1),\n",
    "            type_intervention=str(row[\"type_intervention\"])\n",
    "        )\n",
    "        reseau.add_infra(inf)\n",
    "\n",
    "    # Phase 0 — amorçage par l’arête minimale si aucune source fournie\n",
    "    sources: Set[str] = set()\n",
    "    if not sources:\n",
    "        best_eid = min(reseau.infras, key=lambda e: reseau.infras[e].difficulte())\n",
    "        best = reseau.infras[best_eid]\n",
    "        reseau.set_repaired(best_eid)\n",
    "        sources.update([best.u, best.v])\n",
    "\n",
    "    # Boucle greedy\n",
    "    ordre_rows, travaux_rows = [], []\n",
    "    cout_cumule, longueur_reparee, etape = 0.0, 0.0, 0\n",
    "    raccordes: Set[str] = set(sources)\n",
    "    pop_par_bat = {b: 1 for b in reseau.batiments}\n",
    "    prises_cumulees = sum(pop_par_bat.get(n, 0) for n in raccordes if n in pop_par_bat)\n",
    "    total_population = sum(pop_par_bat.values())\n",
    "\n",
    "    # Log phase 0\n",
    "    for b in sorted(raccordes):\n",
    "        ordre_rows.append({\"rang\": etape, \"batiment\": b, \"cout_incremental\": 0.0,\n",
    "                           \"cout_cumule\": cout_cumule, \"prises_cumulees\": prises_cumulees,\n",
    "                           \"aretes_reparees\": []})\n",
    "\n",
    "    while True:\n",
    "        candidats = set(reseau.batiments.keys()) - raccordes\n",
    "        if not candidats: break\n",
    "        dist_info = reseau.multi_source_shortest_path(raccordes)\n",
    "        dist_info = {n:v for n,v in dist_info.items() if n in candidats}\n",
    "        if not dist_info: break\n",
    "\n",
    "        def key_fn(item):\n",
    "            n,(c,p) = item\n",
    "            return (c, len(p), str(n))\n",
    "        n_best, (cost_best, path_best) = min(dist_info.items(), key=key_fn)\n",
    "        if BUDGET is not None and (cout_cumule + cost_best) > BUDGET: break\n",
    "\n",
    "        etape += 1\n",
    "        repaired_eids = []\n",
    "        for u, v in zip(path_best[:-1], path_best[1:]):\n",
    "            eid = reseau.G[u][v][\"eid\"]\n",
    "            if not reseau.infras[eid].repaired:\n",
    "                reseau.set_repaired(eid)\n",
    "                repaired_eids.append(eid)\n",
    "                longueur_reparee += reseau.infras[eid].longueur\n",
    "\n",
    "        cout_cumule += cost_best\n",
    "        raccordes.add(n_best)\n",
    "        prises_cumulees += pop_par_bat.get(n_best, 1)\n",
    "        ordre_rows.append({\"rang\": etape, \"batiment\": n_best, \"cout_incremental\": round(cost_best,3),\n",
    "                           \"cout_cumule\": round(cout_cumule,3), \"prises_cumulees\": int(prises_cumulees),\n",
    "                           \"aretes_reparees\": repaired_eids})\n",
    "        for eid in repaired_eids:\n",
    "            inf = reseau.infras[eid]\n",
    "            travaux_rows.append({\"etape\": etape, \"eid\": eid, \"u\": inf.u, \"v\": inf.v,\n",
    "                                 \"longueur\": inf.longueur, \"cout\": inf.cout,\n",
    "                                 \"etat_initial\": inf.etat, \"type_intervention\": inf.type_intervention})\n",
    "\n",
    "    df_ordre = pd.DataFrame(ordre_rows)\n",
    "    df_travaux = pd.DataFrame(travaux_rows)\n",
    "    kpis = {\n",
    "        \"cout_total\": float(round(cout_cumule, 3)),\n",
    "        \"prises_total\": int(total_population),\n",
    "        \"prises_reconnectees\": int(prises_cumulees),\n",
    "        \"taux_reconnexion\": float(round((prises_cumulees/total_population*100),2)) if total_population else None,\n",
    "        \"longueur_reparee\": float(round(longueur_reparee,2)),\n",
    "        \"nb_etapes\": int(etape)\n",
    "    }\n",
    "    return df_ordre, df_travaux, kpis, edges\n",
    "\n",
    "\n",
    "def run_pipeline_bipartite(edges: pd.DataFrame):\n",
    "    # colonnes minimales garanties\n",
    "    if \"nb_maisons\" not in edges.columns: edges[\"nb_maisons\"] = 1\n",
    "    if \"longueur\" not in edges.columns: edges[\"longueur\"] = 1.0\n",
    "    if \"infra_type\" not in edges.columns: edges[\"infra_type\"] = \"ligne\"\n",
    "    if \"etat\" not in edges.columns: edges[\"etat\"] = \"intacte\"\n",
    "    edges[\"etat\"] = edges[\"etat\"].map(normalize_etat)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    # noeuds B:<id> pour bâtiments, I:<id> pour infrastructures\n",
    "    for _, r in edges.iterrows():\n",
    "        b = f\"B:{r['id_batiment']}\"; i = f\"I:{r['infra_id']}\"\n",
    "        if b not in G: G.add_node(b, kind=\"batiment\", nb_maisons=int(r[\"nb_maisons\"]))\n",
    "        if i not in G: G.add_node(i, kind=\"infra\", infra_type=str(r[\"infra_type\"]), etat=str(r[\"etat\"]))\n",
    "        # arête B-I (poids = coût proxy)\n",
    "        G.add_edge(b, i,\n",
    "                   longueur=float(r[\"longueur\"]),\n",
    "                   cout=float(r[\"longueur\"]) * COUT_UNITAIRE,\n",
    "                   repaired=False)\n",
    "\n",
    "    def edge_difficulte(u, v) -> float:\n",
    "        data = G[u][v]\n",
    "        cout = data.get(\"cout\", data.get(\"longueur\", 1.0)*COUT_UNITAIRE)\n",
    "        # état depuis le nœud infra\n",
    "        etat = G.nodes[u].get(\"etat\") if str(u).startswith(\"I:\") else G.nodes[v].get(\"etat\")\n",
    "        if data.get(\"repaired\", False): return 0.0\n",
    "        return cout * (1 + (DELTA_REMPLACEMENT if normalize_etat(etat) == \"a_remplacer\" else 0.0))\n",
    "\n",
    "    # Amorçage : arête (B-I) la moins coûteuse\n",
    "    min_edge = min(G.edges(), key=lambda e: edge_difficulte(e[0], e[1]))\n",
    "    u0, v0 = min_edge\n",
    "    G[u0][v0][\"repaired\"] = True\n",
    "    sources: Set[str] = {u0, v0}\n",
    "\n",
    "    def dijkstra_from_sources(G, sources: Set[str]):\n",
    "        H = nx.Graph()\n",
    "        for u, v in G.edges():\n",
    "            H.add_edge(u, v, weight=edge_difficulte(u, v))\n",
    "        SUP = \"__SUP__\"\n",
    "        for s in sources: H.add_edge(SUP, s, weight=0.0)\n",
    "        dist, paths = nx.single_source_dijkstra(H, SUP, weight=\"weight\")\n",
    "        dist.pop(SUP, None); paths.pop(SUP, None)\n",
    "        for n, p in list(paths.items()):\n",
    "            if p and p[0] == SUP: paths[n] = p[1:]\n",
    "        return dist, paths\n",
    "\n",
    "    def path_incremental_cost(G, path: List[str]) -> Tuple[float, List[Tuple[str,str]]]:\n",
    "        total, repaired_edges = 0.0, []\n",
    "        for u, v in zip(path[:-1], path[1:]):\n",
    "            if not G[u][v].get(\"repaired\", False):\n",
    "                total += edge_difficulte(u, v)\n",
    "                repaired_edges.append((u, v))\n",
    "        return total, repaired_edges\n",
    "\n",
    "    batiments = [n for n, d in G.nodes(data=True) if d.get(\"kind\") == \"batiment\"]\n",
    "    pop_par_bat = {b: G.nodes[b].get(\"nb_maisons\",1) for b in batiments}\n",
    "    raccordes: Set[str] = set(sources)\n",
    "    etape, cout_cumule, longueur_reparee = 0, 0.0, 0.0\n",
    "\n",
    "    prises_cumulees = sum(pop_par_bat.get(n,0) for n in raccordes if n in pop_par_bat)\n",
    "    total_population = int(sum(pop_par_bat.values()))\n",
    "\n",
    "    ordre_rows, travaux_rows = [], []\n",
    "    # phase 0\n",
    "    for n in sorted(raccordes):\n",
    "        ordre_rows.append({\"rang\": etape, \"batiment\": n if n.startswith(\"B:\") else None,\n",
    "                           \"cout_incremental\": 0.0, \"cout_cumule\": cout_cumule,\n",
    "                           \"prises_cumulees\": int(prises_cumulees), \"aretes_reparees\": []})\n",
    "\n",
    "    # boucle\n",
    "    while True:\n",
    "        candidats = set(batiments) - raccordes\n",
    "        if not candidats: break\n",
    "        dist, paths = dijkstra_from_sources(G, raccordes)\n",
    "        cand_info = {b:(dist.get(b,np.inf), paths.get(b,[])) for b in candidats}\n",
    "        cand_info = {b:t for b,t in cand_info.items() if np.isfinite(t[0])}\n",
    "        if not cand_info: break\n",
    "\n",
    "        def key_fn(item):\n",
    "            b,(c,p) = item\n",
    "            return (c, len(p), str(b))\n",
    "        b_best,(c_best,p_best) = min(cand_info.items(), key=key_fn)\n",
    "        if BUDGET is not None and (cout_cumule + c_best) > BUDGET: break\n",
    "\n",
    "        etape += 1\n",
    "        repaired_edges = []\n",
    "        for u,v in zip(p_best[:-1], p_best[1:]):\n",
    "            if not G[u][v].get(\"repaired\", False):\n",
    "                G[u][v][\"repaired\"] = True\n",
    "                repaired_edges.append((u,v))\n",
    "                longueur_reparee += float(G[u][v].get(\"longueur\",0.0))\n",
    "\n",
    "        cout_cumule += c_best\n",
    "        raccordes.add(b_best)\n",
    "        prises_cumulees += int(pop_par_bat.get(b_best,0))\n",
    "\n",
    "        ordre_rows.append({\"rang\": etape, \"batiment\": b_best, \"cout_incremental\": float(round(c_best,3)),\n",
    "                           \"cout_cumule\": float(round(cout_cumule,3)), \"prises_cumulees\": int(prises_cumulees),\n",
    "                           \"aretes_reparees\": repaired_edges})\n",
    "        for (u,v) in repaired_edges:\n",
    "            infra_node = u if str(u).startswith(\"I:\") else (v if str(v).startswith(\"I:\") else None)\n",
    "            etat_initial = G.nodes[infra_node].get(\"etat\",\"intacte\") if infra_node else \"intacte\"\n",
    "            type_interv = \"remplacement\" if normalize_etat(etat_initial)==\"a_remplacer\" else \"pose/reutilisation\"\n",
    "            travaux_rows.append({\"etape\": etape, \"u\": u, \"v\": v,\n",
    "                                 \"longueur\": float(G[u][v].get(\"longueur\",0.0)),\n",
    "                                 \"cout_proxy\": float(G[u][v].get(\"cout\",0.0)),\n",
    "                                 \"etat_initial\": etat_initial, \"type_intervention\": type_interv})\n",
    "\n",
    "    df_ordre = pd.DataFrame(ordre_rows)\n",
    "    df_travaux = pd.DataFrame(travaux_rows)\n",
    "    kpis = {\n",
    "        \"cout_total_proxy\": float(round(cout_cumule,3)),\n",
    "        \"prises_total\": int(total_population),\n",
    "        \"prises_reconnectees\": int(prises_cumulees),\n",
    "        \"taux_reconnexion\": float(round((prises_cumulees/total_population*100),2)) if total_population else None,\n",
    "        \"longueur_reparee_totale\": float(round(longueur_reparee,2)),\n",
    "        \"nb_etapes\": int(etape)\n",
    "    }\n",
    "    return df_ordre, df_travaux, kpis, edges\n",
    "\n",
    "# ---------------------------\n",
    "# Exécution selon schéma détecté\n",
    "# ---------------------------\n",
    "if has_edge_list:\n",
    "    df_ordre, df_travaux, kpis, df_entree = run_pipeline_edge_list(edges_raw.copy())\n",
    "else:\n",
    "    df_ordre, df_travaux, kpis, df_entree = run_pipeline_bipartite(edges_raw.copy())\n",
    "\n",
    "# ---------------------------\n",
    "# Exports & aperçu\n",
    "# ---------------------------\n",
    "df_ordre.to_csv(OUT_ORDRE, index=False)\n",
    "df_travaux.to_csv(OUT_TRAVAUX, index=False)\n",
    "with open(OUT_KPIS, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(kpis, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\") as xw:\n",
    "    df_entree.to_excel(xw, index=False, sheet_name=\"entree\")\n",
    "    df_ordre.to_excel(xw, index=False, sheet_name=\"ordre_raccordement\")\n",
    "    df_travaux.to_excel(xw, index=False, sheet_name=\"travaux_par_etape\")\n",
    "\n",
    "print(\"KPIs:\", kpis)\n",
    "print(\"Exports :\")\n",
    "print(\" -\", OUT_ORDRE)\n",
    "print(\" -\", OUT_TRAVAUX)\n",
    "print(\" -\", OUT_KPIS)\n",
    "print(\" -\", OUT_XLSX)\n",
    "\n",
    "# (Option) afficher un aperçu dans le notebook\n",
    "try:\n",
    "    display(df_ordre.head(10))\n",
    "    display(df_travaux.head(10))\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a702373-cc29-46fd-b13d-6b8052c0ac56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlsxwriter\n",
      "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\micromedia\\appdata\\roaming\\python\\python313\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\micromedia\\appdata\\roaming\\python\\python313\\site-packages (from openpyxl) (2.0.0)\n",
      "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
      "Installing collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.2.9\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a4de24-5c1c-40a6-92ea-a5962b041ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
